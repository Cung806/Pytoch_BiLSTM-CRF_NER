{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embeded_path = \"E:/data/NER_dataset/word2vec/cn_char_fastnlp_100d.txt\"\n",
    "train_path = \"E:/data/NER_dataset/Weibo/weiboNER_2nd_conll.train\"\n",
    "test_path = \"E:/data/NER_dataset/Weibo/weiboNER_2nd_conll.test\"\n",
    "dev_path = \"E:/data/NER_dataset/Weibo/weiboNER_2nd_conll.dev\"\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCH = 6\n",
    "embedding_size = 100\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(data_dir):\n",
    "    word_lists = []\n",
    "    tag_lists = []\n",
    "    with open(data_dir, 'r', encoding='utf-8') as f:\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                word, tag = line.strip(\"\\n\").split()\n",
    "                word = word[0]\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "            else:\n",
    "                word_lists.append(words)\n",
    "                tag_lists.append(tags)\n",
    "                words = []\n",
    "                tags = []\n",
    "    return word_lists, tag_lists\n",
    "\n",
    "train_words_lists, train_tag_lists = build_corpus(train_path)\n",
    "test_words_lists, test_tag_lists = build_corpus(test_path)\n",
    "dev_words_lists, dev_tag_lists = build_corpus(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag(train_words_lists, test_words_lists,dev_words_lists, train_tag_lists):\n",
    "    words_lists = []\n",
    "    words_lists.extend(train_words_lists)\n",
    "    words_lists.extend(test_words_lists)\n",
    "    words_lists.extend(dev_words_lists)\n",
    "    words_map = {}\n",
    "    for list in words_lists:\n",
    "        for e in list:\n",
    "            if e not in words_map:\n",
    "                words_map[e] = len(words_map)+2\n",
    "    words_map['<pad>'] = 0\n",
    "    words_map['<unk>'] = 1\n",
    "    \n",
    "    id2word = {}\n",
    "    for x in words_map:\n",
    "        id2word[words_map[x]] = x\n",
    "    \n",
    "    tags_map = {}\n",
    "    for list in train_tag_lists:\n",
    "        for e in list:\n",
    "            if e not in tags_map:\n",
    "                tags_map[e] = len(tags_map)\n",
    "    id2tag = {}\n",
    "    for x in tags_map:\n",
    "        id2tag[tags_map[x]] = x\n",
    "    return words_map,id2word, tags_map, id2tag\n",
    "\n",
    "## 得到单词和标签到id的映射\n",
    "word2id, id2word, tag2id, id2tag = get_word_tag(train_words_lists, test_words_lists, dev_words_lists, train_tag_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "方法1：\n",
    "将数据集进行截断和填充，得到每一个batch的大小都一样\n",
    "'''\n",
    "\n",
    "## 将单词映射到id\n",
    "def tokenize2id(words_list, tag_list, word2id, tag2id):\n",
    "    words2id_list = []\n",
    "    tags2id_list = []\n",
    "    for i in range(len(words_list)):\n",
    "        words2id_list.append(list(map(lambda x: word2id[x], words_list[i] )))\n",
    "        tags2id_list.append(list(map(lambda x: tag2id[x], tag_list[i])))\n",
    "    return  words2id_list, tags2id_list\n",
    "\n",
    "train_words_id, train_tags_id = tokenize2id(train_words_lists, train_tag_lists, word2id, tag2id)\n",
    "# test_words_id, test_tags_id = tokenize2id(test_words_lists, test_tag_lists, word2id, tag2id)\n",
    "# dev_words_id, dev_tags_id = tokenize2id(dev_words_lists, dev_tag_lists, word2id, tag2id)\n",
    "\n",
    "## 将语料进行对齐和截断\n",
    "def get_padded_seq(words, tags):\n",
    "    padded_wordsid = pad_sequences(words, maxlen= MAX_SEQ_LEN, dtype=\"long\",\n",
    "              truncating='post', padding='post')\n",
    "    padded_tagsid = pad_sequences(tags, maxlen= MAX_SEQ_LEN, dtype=\"long\",\n",
    "                                   truncating='post', padding='post')\n",
    "    padded_wordsid = torch.LongTensor(padded_wordsid)\n",
    "    padded_tagsid = torch.LongTensor(padded_tagsid)\n",
    "    return padded_wordsid, padded_tagsid\n",
    "\n",
    "train_x, train_y = get_padded_seq(train_words_id, train_tags_id)\n",
    "# test_x, test_y = get_padded_seq(test_words_id, test_tags_id)\n",
    "# dev_x, dev_y = get_padded_seq(dev_words_id, dev_tags_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "方法2, 由于直接对测试集进行截断，会损失一部分预测的值，因此不能截断，只能去每一个batch里面的最大长度\n",
    "利用collect_fn,让每一个batch的大小都和最大的长度一样\n",
    "'''\n",
    "\n",
    "## 将单词映射到id\n",
    "def tokenize2id(words_list, tag_list, word2id, tag2id):\n",
    "    words2id_list = []\n",
    "    tags2id_list = []\n",
    "    for i in range(len(words_list)):\n",
    "        words2id_list.append(torch.tensor(list(map(lambda x: word2id[x], words_list[i] ))))\n",
    "        tags2id_list.append(torch.tensor(list(map(lambda x: tag2id[x], tag_list[i]))))\n",
    "    return  words2id_list, tags2id_list\n",
    "\n",
    "\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x), reverse=True)\n",
    "    data = rnn_utils.pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    return data\n",
    "\n",
    "# train_words_id, train_tags_id = tokenize2id(train_words_lists, train_tag_lists, word2id, tag2id)\n",
    "test_words_id, test_tags_id = tokenize2id(test_words_lists, test_tag_lists, word2id, tag2id)\n",
    "dev_words_id, dev_tags_id = tokenize2id(dev_words_lists, dev_tag_lists, word2id, tag2id)\n",
    "\n",
    "\n",
    "test_x = Data.DataLoader(test_words_id, batch_size=3, shuffle=False, \n",
    "                             collate_fn=collate_fn)\n",
    "test_y = Data.DataLoader(test_tags_id, batch_size=3, shuffle=False, \n",
    "                             collate_fn=collate_fn)\n",
    "dev_x = Data.DataLoader(dev_words_id, batch_size=3, shuffle=False, \n",
    "                             collate_fn=collate_fn)\n",
    "dev_y = Data.DataLoader(dev_tags_id, batch_size=3, shuffle=False, \n",
    "                             collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 将语料进行对齐和截断\n",
    "# def get_padded_seq(words, tags):\n",
    "#     padded_wordsid = pad_sequences(words, maxlen= MAX_SEQ_LEN, dtype=\"long\",\n",
    "#               truncating='post', padding='post')\n",
    "#     padded_tagsid = pad_sequences(tags, maxlen= MAX_SEQ_LEN, dtype=\"long\",\n",
    "#                                    truncating='post', padding='post')\n",
    "#     padded_wordsid = torch.LongTensor(padded_wordsid)\n",
    "#     padded_tagsid = torch.LongTensor(padded_tagsid)\n",
    "#     return padded_wordsid, padded_tagsid\n",
    "\n",
    "# train_x, train_y = get_padded_seq(train_words_id, train_tags_id)\n",
    "# test_x, test_y = get_padded_seq(test_words_id, test_tags_id)\n",
    "# dev_x, dev_y = get_padded_seq(dev_words_id, dev_tags_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data.TensorDataset(train_x, train_y)\n",
    "# test_dataset = Data.TensorDataset(test_x, test_y)\n",
    "# dev_dataset = Data.TensorDataset(dev_x, dev_y)\n",
    "train_loader = Data.DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            shuffle = True)\n",
    "# test_loader = Data.DataLoader(\n",
    "#         dataset=test_dataset,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=True)\n",
    "# dev_loader = Data.DataLoader(\n",
    "#         dataset=dev_dataset,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    }
   ],
   "source": [
    "## 读取词向量\n",
    "def pretrained_embedding(embed_path):\n",
    "    tmp_file = get_tmpfile(embed_path)\n",
    "    wvmodel = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    embed_size = len(wvmodel.get_vector(wvmodel.index2word[3]))\n",
    "    vocab_size = len(word2id)\n",
    "\n",
    "    weight = torch.zeros(vocab_size, embed_size)\n",
    "    for i in range(vocab_size):\n",
    "        try:\n",
    "            index = word2id[wvmodel.index2word[i]]\n",
    "        except:\n",
    "            continue\n",
    "        weight[index,:] = torch.from_numpy(wvmodel.get_vector(\n",
    "            wvmodel.index2word[i]\n",
    "        ))\n",
    "    return weight\n",
    "weight = pretrained_embedding(embeded_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, out_size, weight=None,drop_out=0.5):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        if weight!=None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.bilstm = nn.LSTM(emb_size, hidden_size,\n",
    "                              batch_first=True,\n",
    "                              bidirectional=True)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.fc = nn.Linear(2*hidden_size, out_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        emb = self.embedding(sentence)  # [B, L, emb_size]\n",
    "#         packed = pack_padded_sequence(emb, lengths, batch_first=True)\n",
    "        lstm_out, _ = self.bilstm(emb)\n",
    "#         lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        scores = self.fc(lstm_out)\n",
    "        scores = self.dropout(scores)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(pred, target):\n",
    "    '''\n",
    "    :param pred:  [Batch_size, seqlen , tagsnum]\n",
    "    :param target:  [Batch_size, seqlen ]\n",
    "    :return:\n",
    "    '''\n",
    "    pred = pred.permute(0,2,1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(pred, target)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def metrics(pred, batch_y):\n",
    "#     print(\"met\",pred.shape,batch_y.shape)\n",
    "    pred = torch.argmax(F.log_softmax(pred,dim=2),dim=2).view(-1).detach().numpy()\n",
    "    target = batch_y.view(-1).numpy()\n",
    "    correct = 0  ## 识别正确的\n",
    "    recongized = 0   ## 识别出来的\n",
    "    real = 0   ### 实际实体数\n",
    "    for i in range(len(pred)):\n",
    "        if target[i]!=0:\n",
    "            real+=1\n",
    "        if target[i] == pred[i] and target[i]!=0:\n",
    "            correct+=1\n",
    "            recongized+=1\n",
    "        elif pred[i]!=0:\n",
    "            recongized+=1\n",
    "#     print(\"met\",pred.shape,target.shape)\n",
    "    return correct, recongized, real\n",
    "\n",
    "\n",
    "def conlleval(pred, batch_y):\n",
    "    pred = list(torch.argmax(F.log_softmax(pred,dim=2),dim=2).view(-1).detach().numpy())\n",
    "    target = list(batch_y.view(-1).numpy())\n",
    "    real = 0\n",
    "    recongnized = 0\n",
    "    correct = 0\n",
    "    target = [0]+ target+ [0] ## 前后增加0 防止数组越界\n",
    "    pred = [0]+ pred+ [0]\n",
    "    for i in range(len(target)-1):\n",
    "        if target[i]!=0 and target[i+1]==0:\n",
    "            real+=1\n",
    "        if pred[i]!=0 and pred[i+1]==0:\n",
    "            recongnized+=1\n",
    "    pred_loc = []   ## 记录预测的实体位置\n",
    "    start,end =1,1\n",
    "    while start<len(pred) and end<len(pred):\n",
    "        if pred[start]!=0 and pred[start-1]==0:\n",
    "            end = start\n",
    "            while pred[end]!=0:\n",
    "                end+=1\n",
    "            if start!=end:\n",
    "                pred_loc.append((start,end))\n",
    "            start = end\n",
    "            start +=1\n",
    "        else:\n",
    "            start+=1\n",
    "    for x in pred_loc:\n",
    "        start,end = x\n",
    "        if target[start:end] == pred[start:end] and target[start-1]==0 and target[end]==0:\n",
    "            correct += 1\n",
    "    return correct, recongnized, real\n",
    "\n",
    "def evaluate(model, data_loader):   \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct_all = 0  ## 识别正确得 总的\n",
    "    recongized_all = 0   ## 识别出来得 总的\n",
    "    real_all = 0   ### 实际样本数 总的\n",
    "    with torch.no_grad():\n",
    "        for step,(batch_x, batch_y) in enumerate(data_loader):\n",
    "            pred = model(batch_x)\n",
    "#             print(\"eva\",pred.shape,batch_y.shape)\n",
    "            loss = cal_loss(pred, batch_y)\n",
    "            epoch_loss+=loss.item()\n",
    "            correct, recongized, real = conlleval(pred, batch_y)\n",
    "            correct_all +=correct\n",
    "            recongized_all+=recongized\n",
    "            real_all+=real\n",
    "    print(correct_all, recongized_all, real_all)\n",
    "    precision = 0 if recongized_all==0 else (correct_all/recongized_all)\n",
    "    recall = 0 if real_all ==0 else (correct_all/real_all)\n",
    "    f1 = 0 if recall + precision == 0 else (2 * precision * recall) / (precision + recall)\n",
    "    print(\"loss:\",epoch_loss,\",||precision:\",precision,',||recall:',recall,\",||F1:\",f1)\n",
    "    \n",
    "def evaluate1(model, x,y):   \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct_all = 0  ## 识别正确得 总的\n",
    "    recongized_all = 0   ## 识别出来得 总的\n",
    "    real_all = 0   ### 实际样本数 总的\n",
    "    with torch.no_grad():\n",
    "        for step,(batch_x, batch_y) in enumerate(zip(x,y)):\n",
    "            pred = model(batch_x)\n",
    "#             print(\"eva\",pred.shape,batch_y.shape)\n",
    "            loss = cal_loss(pred, batch_y)\n",
    "            epoch_loss+=loss.item()\n",
    "            correct, recongized, real = conlleval(pred, batch_y)\n",
    "            correct_all +=correct\n",
    "            recongized_all+=recongized\n",
    "            real_all+=real\n",
    "    print(correct_all, recongized_all, real_all)\n",
    "    precision = 0 if recongized_all==0 else (correct_all/recongized_all)\n",
    "    recall = 0 if real_all ==0 else (correct_all/real_all)\n",
    "    f1 = 0 if recall + precision == 0 else (2 * precision * recall) / (precision + recall)\n",
    "    print(\"loss:\",epoch_loss,\",||precision:\",precision,',||recall:',recall,\",||F1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "31 271 375\n",
      "loss: 23.585842177271843 ,||precision: 0.11439114391143912 ,||recall: 0.08266666666666667 ,||F1: 0.09597523219814243\n",
      "epoch: 1\n",
      "29 324 375\n",
      "loss: 19.298527613282204 ,||precision: 0.08950617283950617 ,||recall: 0.07733333333333334 ,||F1: 0.08297567954220315\n",
      "epoch: 2\n",
      "62 289 375\n",
      "loss: 15.843255652114749 ,||precision: 0.21453287197231835 ,||recall: 0.16533333333333333 ,||F1: 0.18674698795180725\n",
      "epoch: 3\n",
      "69 384 375\n",
      "loss: 16.602137187495828 ,||precision: 0.1796875 ,||recall: 0.184 ,||F1: 0.18181818181818182\n",
      "epoch: 4\n",
      "82 346 375\n",
      "loss: 15.154858459718525 ,||precision: 0.23699421965317918 ,||recall: 0.21866666666666668 ,||F1: 0.22746185852981968\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BI-LSTM  train\n",
    "\"\"\"\n",
    "vocab_size = len(word2id)\n",
    "out_size = len(tag2id)\n",
    "model = BiLSTM(vocab_size, embedding_size, hidden_size, out_size, weight=None)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(5):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_x)\n",
    "        loss = cal_loss(predictions, batch_y)\n",
    "        loss.backward()\n",
    "        train_loss+=loss.item()\n",
    "        optimizer.step()\n",
    "    print(\"epoch:\",epoch)\n",
    "    evaluate1(model, test_x, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
