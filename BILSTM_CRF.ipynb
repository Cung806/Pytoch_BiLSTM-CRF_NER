{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embeded_path = \"E:/data/NER_dataset/word2vec/cn_char_fastnlp_100d.txt\"\n",
    "# train_path = \"E:/data/NER_dataset/Weibo/weiboNER_2nd_conll.train\"\n",
    "# test_path = \"E:/data/NER_dataset/Weibo/weiboNER_2nd_conll.test\"\n",
    "# dev_path = \"E:/data/NER_dataset/Weibo/weiboNER_2nd_conll.dev\"\n",
    "train_path = \"E:/data/NER_dataset/Weibo/NAM/weibo_train_NAM.txt\"\n",
    "test_path = \"E:/data/NER_dataset/Weibo/NAM/weibo_test_NAM.txt\"\n",
    "dev_path = \"E:/data/NER_dataset/Weibo/NAM/weibo_dev_NAM.txt\"\n",
    "MAX_SEQ_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 6\n",
    "embedding_size = 100\n",
    "hidden_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(data_dir):\n",
    "    word_lists = []\n",
    "    tag_lists = []\n",
    "    with open(data_dir, 'r', encoding='utf-8') as f:\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                word, tag = line.strip(\"\\n\").split()\n",
    "#                 word = word[0]\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "            else:\n",
    "                word_lists.append(words)\n",
    "                tag_lists.append(tags)\n",
    "                words = []\n",
    "                tags = []\n",
    "    return word_lists, tag_lists\n",
    "\n",
    "train_words_lists, train_tag_lists = build_corpus(train_path)\n",
    "test_words_lists, test_tag_lists = build_corpus(test_path)\n",
    "dev_words_lists, dev_tag_lists = build_corpus(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag(train_words_lists, test_words_lists,dev_words_lists, train_tag_lists):\n",
    "    words_lists = []\n",
    "    words_lists.extend(train_words_lists)\n",
    "    words_lists.extend(test_words_lists)\n",
    "    words_lists.extend(dev_words_lists)\n",
    "    words_map = {}\n",
    "    for list in words_lists:\n",
    "        for e in list:\n",
    "            if e not in words_map:\n",
    "                words_map[e] = len(words_map)+2\n",
    "    words_map['<pad>'] = 0\n",
    "    words_map['<unk>'] = 1\n",
    "    \n",
    "    id2word = {}\n",
    "    for x in words_map:\n",
    "        id2word[words_map[x]] = x\n",
    "    \n",
    "    tags_map = {}\n",
    "    for list in train_tag_lists:\n",
    "        for e in list:\n",
    "            if e not in tags_map:\n",
    "                tags_map[e] = len(tags_map)\n",
    "    id2tag = {}\n",
    "    for x in tags_map:\n",
    "        id2tag[tags_map[x]] = x\n",
    "    return words_map,id2word, tags_map, id2tag\n",
    "\n",
    "## 得到单词和标签到id的映射\n",
    "word2id, id2word, tag2id, id2tag = get_word_tag(train_words_lists, test_words_lists, dev_words_lists, train_tag_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "方法1：\n",
    "将数据集进行截断和填充，得到每一个batch的大小都一样\n",
    "'''\n",
    "\n",
    "## 将单词映射到id\n",
    "def tokenize2id(words_list, tag_list, word2id, tag2id):\n",
    "    words2id_list = []\n",
    "    tags2id_list = []\n",
    "    for i in range(len(words_list)):\n",
    "        words2id_list.append(list(map(lambda x: word2id[x], words_list[i] )))\n",
    "        tags2id_list.append(list(map(lambda x: tag2id[x], tag_list[i])))\n",
    "    return  words2id_list, tags2id_list\n",
    "\n",
    "train_words_id, train_tags_id = tokenize2id(train_words_lists, train_tag_lists, word2id, tag2id)\n",
    "# test_words_id, test_tags_id = tokenize2id(test_words_lists, test_tag_lists, word2id, tag2id)\n",
    "# dev_words_id, dev_tags_id = tokenize2id(dev_words_lists, dev_tag_lists, word2id, tag2id)\n",
    "\n",
    "## 将语料进行对齐和截断\n",
    "def get_padded_seq(words, tags):\n",
    "    padded_wordsid = pad_sequences(words, maxlen= MAX_SEQ_LEN, dtype=\"long\",\n",
    "              truncating='post', padding='post')\n",
    "    padded_tagsid = pad_sequences(tags, maxlen= MAX_SEQ_LEN, dtype=\"long\",\n",
    "                                   truncating='post', padding='post')\n",
    "    masks = torch.zeros(len(words),MAX_SEQ_LEN).type(torch.ByteTensor)\n",
    "    for i,x in enumerate(padded_wordsid):\n",
    "        if 0 in x:\n",
    "            zero_index = list(x).index(0)\n",
    "            masks[i,:zero_index] = torch.ones(zero_index)\n",
    "        else:\n",
    "            masks[i] = torch.ones(MAX_SEQ_LEN)\n",
    "\n",
    "    padded_wordsid = torch.LongTensor(padded_wordsid)\n",
    "    padded_tagsid = torch.LongTensor(padded_tagsid)\n",
    "#     lengths = torch.LongTensor(lengths)\n",
    "    return padded_wordsid, padded_tagsid, masks\n",
    "\n",
    "train_x, train_y, train_mask = get_padded_seq(train_words_id, train_tags_id)\n",
    "# test_x, test_y = get_padded_seq(test_words_id, test_tags_id)\n",
    "# dev_x, dev_y = get_padded_seq(dev_words_id, dev_tags_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "方法2, 由于直接对test测试集进行截断，会损失一部分预测的值，因此不能截断，只能去每一个batch里面的最大长度\n",
    "利用collate_fn,让每一个batch的大小都和最大的长度一样\n",
    "'''\n",
    "\n",
    "## 将单词映射到id\n",
    "def tokenize2id(words_list, tag_list, word2id, tag2id):\n",
    "    words2id_list = []\n",
    "    tags2id_list = []\n",
    "    for i in range(len(words_list)):\n",
    "        words2id_list.append(torch.tensor(list(map(lambda x: word2id[x], words_list[i] ))))\n",
    "        tags2id_list.append(torch.tensor(list(map(lambda x: tag2id[x], tag_list[i]))))\n",
    "    return  words2id_list, tags2id_list\n",
    "\n",
    "\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "def collate_fn(data):\n",
    "#     data.sort(key=lambda x: len(x), reverse=True)\n",
    "    data = rnn_utils.pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    return data\n",
    "\n",
    "# train_words_id, train_tags_id = tokenize2id(train_words_lists, train_tag_lists, word2id, tag2id)\n",
    "test_words_id, test_tags_id = tokenize2id(test_words_lists, test_tag_lists, word2id, tag2id)\n",
    "dev_words_id, dev_tags_id = tokenize2id(dev_words_lists, dev_tag_lists, word2id, tag2id)\n",
    "\n",
    "\n",
    "def get_mask(words_id):\n",
    "    test_mask = []\n",
    "    for e in words_id:\n",
    "        elen = len(e)\n",
    "        t = torch.ones(elen).type(torch.ByteTensor)\n",
    "        test_mask.append(t)\n",
    "    return test_mask\n",
    "\n",
    "test_masks = get_mask(test_words_id)\n",
    "dev_masks = get_mask(dev_words_id)\n",
    "\n",
    "test_x = Data.DataLoader(test_words_id, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                             collate_fn=collate_fn)\n",
    "test_y = Data.DataLoader(test_tags_id, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                             collate_fn=collate_fn)\n",
    "dev_x = Data.DataLoader(dev_words_id, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                             collate_fn=collate_fn)\n",
    "dev_y = Data.DataLoader(dev_tags_id, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                             collate_fn=collate_fn)\n",
    "test_masks = Data.DataLoader(test_masks, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                             collate_fn=collate_fn)\n",
    "dev_masks = Data.DataLoader(dev_masks, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                             collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data.TensorDataset(train_x, train_y, train_mask)\n",
    "# test_dataset = Data.TensorDataset(test_x, test_y)\n",
    "# dev_dataset = Data.TensorDataset(dev_x, dev_y)\n",
    "train_loader = Data.DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            shuffle = True)\n",
    "# test_loader = Data.DataLoader(\n",
    "#         dataset=test_dataset,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=True)\n",
    "# dev_loader = Data.DataLoader(\n",
    "#         dataset=dev_dataset,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    }
   ],
   "source": [
    "## 读取词向量\n",
    "def pretrained_embedding(embed_path):\n",
    "    tmp_file = get_tmpfile(embed_path)\n",
    "    wvmodel = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    embed_size = len(wvmodel.get_vector(wvmodel.index2word[3]))\n",
    "    vocab_size = len(word2id)\n",
    "\n",
    "    weight = torch.randn(vocab_size, embed_size)\n",
    "    for i in range(vocab_size):\n",
    "        try:\n",
    "            index = word2id[wvmodel.index2word[i]]\n",
    "        except:\n",
    "            continue\n",
    "        weight[index,:] = torch.from_numpy(wvmodel.get_vector(\n",
    "            wvmodel.index2word[i]\n",
    "        ))\n",
    "    return weight\n",
    "weight = pretrained_embedding(embeded_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "## reference : https://github.com/kmkurn/pytorch-crf/issues/40\n",
    "class BiLSTMCRF(nn.Module):\n",
    "    def __init__(self, vocab_size, num_tags, emb_size, hidden_size, weight=None,drop_out=0.5):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        if weight!=None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.bilstm = nn.LSTM(emb_size, hidden_size,\n",
    "                              batch_first=True,\n",
    "                              bidirectional=True)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.fc = nn.Linear(2*hidden_size, num_tags)\n",
    "        self.crf = CRF(num_tags,batch_first =True)\n",
    "\n",
    "        \n",
    "    def neglikelihood(self, sentence, batch_y, masks):\n",
    "        feats = self.get_lstm_features(sentence)\n",
    "        crf_loss = self.crf(feats, batch_y, mask = masks)\n",
    "        return -crf_loss\n",
    "        \n",
    "    def get_lstm_features(self, sentence):\n",
    "        emb = self.embedding(sentence)  # [B, L, emb_size]\n",
    "        lstm_out, _ = self.bilstm(emb)\n",
    "        scores = self.fc(lstm_out)\n",
    "        scores = self.dropout(scores)\n",
    "        return scores\n",
    "        \n",
    "    def forward(self, sentence, masks):\n",
    "        emb = self.embedding(sentence)  # [B, L, emb_size]\n",
    "        lstm_out, _ = self.bilstm(emb)\n",
    "        scores = self.fc(lstm_out)\n",
    "        scores = self.dropout(scores)\n",
    "        ## 不需要对scores做softmax\n",
    "#         scores = F.log_softmax(scores, dim=-1)\n",
    "        crf_out = self.crf.decode(scores, mask =masks)\n",
    "        \n",
    "        return crf_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conlleval(predictions, batch_y):\n",
    "    pred = []\n",
    "    target = []\n",
    "    for i,p in enumerate(predictions):\n",
    "        pred.extend(p)\n",
    "        target.extend(list(batch_y[i][:len(p)]))\n",
    "    real = 0\n",
    "    recongnized = 0\n",
    "    correct = 0\n",
    "    target = [0]+ target+ [0] ## 前后增加0 防止数组越界\n",
    "    pred = [0]+ pred+ [0]\n",
    "    for i in range(len(target)-1):\n",
    "        if target[i]!=0 and target[i+1]==0:\n",
    "            real+=1\n",
    "        if pred[i]!=0 and pred[i+1]==0:\n",
    "            recongnized+=1\n",
    "    pred_loc = []   ## 记录预测的实体位置\n",
    "    start,end =1,1\n",
    "    while start<len(pred) and end<len(pred):\n",
    "        if pred[start]!=0 and pred[start-1]==0:\n",
    "            end = start\n",
    "            while pred[end]!=0:\n",
    "                end+=1\n",
    "            if start!=end:\n",
    "                pred_loc.append((start,end))\n",
    "            start = end\n",
    "            start +=1\n",
    "        else:\n",
    "            start+=1\n",
    "    for x in pred_loc:\n",
    "        start,end = x\n",
    "        if target[start:end] == pred[start:end] and target[start-1]==0 and target[end]==0:\n",
    "            correct += 1\n",
    "    return correct, recongnized, real\n",
    "\n",
    "def evaluate(model, x, y, masks):   \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct_all = 0  ## 识别正确得 总的\n",
    "    recongized_all = 0   ## 识别出来得 总的\n",
    "    real_all = 0   ### 实际实体数 总的\n",
    "    with torch.no_grad():\n",
    "        for step,(batch_x, batch_y,mask) in enumerate(zip(x,y,masks)):\n",
    "            pred = model(batch_x,mask)\n",
    "#             print(\"eva\",pred.shape,batch_y.shape)\n",
    "            loss = model.neglikelihood(batch_x, batch_y, mask)\n",
    "            epoch_loss+=loss.item()\n",
    "            correct, recongized, real = conlleval(pred, batch_y)\n",
    "            correct_all +=correct\n",
    "            recongized_all+=recongized\n",
    "            real_all+=real\n",
    "#     print(correct_all, recongized_all, real_all)\n",
    "    precision = 0 if recongized_all==0 else (correct_all/recongized_all)\n",
    "    recall = 0 if real_all ==0 else (correct_all/real_all)\n",
    "    f1 = 0 if recall + precision == 0 else (2 * precision * recall) / (precision + recall)\n",
    "    print(\"loss:\",epoch_loss,\",||precision:\",precision,',||recall:',recall,\",||F1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: 62990.64846801758\n",
      "loss: 2874.759735107422 ,||precision: 0.0 ,||recall: 0.0 ,||F1: 0\n",
      "epoch: 1 train_loss: 32930.607482910156\n",
      "loss: 2155.957633972168 ,||precision: 0.5 ,||recall: 0.004975124378109453 ,||F1: 0.009852216748768473\n",
      "epoch: 2 train_loss: 19250.55931854248\n",
      "loss: 1769.231430053711 ,||precision: 0.8333333333333334 ,||recall: 0.04975124378109453 ,||F1: 0.09389671361502348\n",
      "epoch: 3 train_loss: 12701.864795684814\n",
      "loss: 1524.0867309570312 ,||precision: 0.5769230769230769 ,||recall: 0.07462686567164178 ,||F1: 0.13215859030837002\n",
      "epoch: 4 train_loss: 9629.169967651367\n",
      "loss: 1388.4080352783203 ,||precision: 0.6666666666666666 ,||recall: 0.08955223880597014 ,||F1: 0.15789473684210525\n",
      "epoch: 5 train_loss: 7677.850845336914\n",
      "loss: 1280.6250762939453 ,||precision: 0.6585365853658537 ,||recall: 0.13432835820895522 ,||F1: 0.2231404958677686\n",
      "epoch: 6 train_loss: 6665.474365234375\n",
      "loss: 1236.2692565917969 ,||precision: 0.5681818181818182 ,||recall: 0.12437810945273632 ,||F1: 0.20408163265306126\n",
      "epoch: 7 train_loss: 5823.356248855591\n",
      "loss: 1174.821517944336 ,||precision: 0.6271186440677966 ,||recall: 0.18407960199004975 ,||F1: 0.28461538461538455\n",
      "epoch: 8 train_loss: 5183.9101486206055\n",
      "loss: 1173.8622436523438 ,||precision: 0.6376811594202898 ,||recall: 0.21890547263681592 ,||F1: 0.32592592592592595\n",
      "epoch: 9 train_loss: 4546.888427734375\n",
      "loss: 1091.5515403747559 ,||precision: 0.6438356164383562 ,||recall: 0.23383084577114427 ,||F1: 0.3430656934306569\n",
      "epoch: 10 train_loss: 4008.8077087402344\n",
      "loss: 1058.09330368042 ,||precision: 0.5346534653465347 ,||recall: 0.26865671641791045 ,||F1: 0.35761589403973515\n",
      "epoch: 11 train_loss: 3691.305793762207\n",
      "loss: 1122.1371459960938 ,||precision: 0.5662650602409639 ,||recall: 0.23383084577114427 ,||F1: 0.33098591549295775\n",
      "epoch: 12 train_loss: 3344.079414367676\n",
      "loss: 1109.540714263916 ,||precision: 0.5641025641025641 ,||recall: 0.3283582089552239 ,||F1: 0.4150943396226415\n",
      "epoch: 13 train_loss: 2996.113594055176\n",
      "loss: 1375.845474243164 ,||precision: 0.686046511627907 ,||recall: 0.2935323383084577 ,||F1: 0.41114982578397213\n",
      "epoch: 14 train_loss: 2832.133716583252\n",
      "loss: 1105.2791366577148 ,||precision: 0.5384615384615384 ,||recall: 0.31343283582089554 ,||F1: 0.3962264150943396\n",
      "epoch: 15 train_loss: 2551.3760528564453\n",
      "loss: 1307.8688659667969 ,||precision: 0.5338983050847458 ,||recall: 0.31343283582089554 ,||F1: 0.3949843260188088\n",
      "epoch: 16 train_loss: 2362.3568534851074\n",
      "loss: 1268.8527221679688 ,||precision: 0.6041666666666666 ,||recall: 0.2885572139303483 ,||F1: 0.39057239057239057\n",
      "epoch: 17 train_loss: 2223.9937591552734\n",
      "loss: 1223.731430053711 ,||precision: 0.4827586206896552 ,||recall: 0.3482587064676617 ,||F1: 0.4046242774566474\n",
      "epoch: 18 train_loss: 2108.0162925720215\n",
      "loss: 1242.543067932129 ,||precision: 0.55 ,||recall: 0.3283582089552239 ,||F1: 0.411214953271028\n",
      "epoch: 19 train_loss: 1990.6877746582031\n",
      "loss: 1234.3593444824219 ,||precision: 0.4897959183673469 ,||recall: 0.3582089552238806 ,||F1: 0.4137931034482758\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BI-LSTM+CRF  train\n",
    "\"\"\"\n",
    "vocab_size = len(word2id)\n",
    "out_size = len(tag2id)\n",
    "crfmodel = BiLSTMCRF(vocab_size,out_size, embedding_size, hidden_size, weight)\n",
    "optimizer = optim.Adam(crfmodel.parameters(), lr=0.015)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(20):\n",
    "    train_loss = 0\n",
    "    crfmodel.train()\n",
    "    for step, (batch_x, batch_y, masks) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = crfmodel(batch_x, masks)\n",
    "        loss = crfmodel.neglikelihood(batch_x, batch_y, masks)\n",
    "        loss.backward()\n",
    "        train_loss+=loss.item()\n",
    "        optimizer.step()\n",
    "    print(\"epoch:\",epoch,\"train_loss:\",train_loss)\n",
    "    evaluate(crfmodel, test_x, test_y, test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n",
      "138\n",
      "146\n",
      "145\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "for x in test_x:\n",
    "    print(len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
